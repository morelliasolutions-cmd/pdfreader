"""
Test Florence-2 pour contr√¥le qualit√© d'une installation FTTH
"""

import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForCausalLM
import time
import sys

# Configuration
MODEL_NAME = "microsoft/Florence-2-base"
device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

# Image √† analyser
IMAGE_PATH = r"C:\Users\etien\OneDrive - Fibernet-S\Desktop\prise\02-04\Photo 27.03.25 14 52 34.jpg"


def patch_florence2_model():
    """Patch pour corriger le probl√®me _supports_sdpa avec Florence-2"""
    try:
        import transformers
        if hasattr(transformers, 'models') and hasattr(transformers.models, 'florence2'):
            try:
                from transformers.models.florence2 import modeling_florence2
                if hasattr(modeling_florence2, 'Florence2ForConditionalGeneration'):
                    if not hasattr(modeling_florence2.Florence2ForConditionalGeneration, '_supports_sdpa'):
                        setattr(modeling_florence2.Florence2ForConditionalGeneration, '_supports_sdpa', False)
            except Exception as e:
                pass
    except Exception as e:
        pass


def analyze_ftth_installation():
    """Analyse compl√®te d'une installation FTTH"""
    print("=" * 80)
    print("üîç CONTR√îLE QUALIT√â - INSTALLATION PRISE OPTIQUE FTTH")
    print("=" * 80)
    print()
    
    # V√©rifier disponibilit√© GPU
    gpu_available = torch.cuda.is_available()
    if gpu_available:
        gpu_name = torch.cuda.get_device_name(0)
        print(f"üéÆ GPU d√©tect√©: {gpu_name}")
    else:
        print("‚ö†Ô∏è  Aucun GPU d√©tect√© - Utilisation du CPU")
    print()
    
    # Avertissement sur les limitations du mod√®le
    print("‚ö†Ô∏è  NOTE IMPORTANTE:")
    print("   Ce mod√®le g√©n√©raliste n'est PAS sp√©cifiquement entra√Æn√© pour les")
    print("   installations FTTH. Pour de meilleurs r√©sultats, un mod√®le fine-tun√©")
    print("   sur des images d'installations FTTH serait n√©cessaire.")
    print()
    
    # Appliquer le patch
    patch_florence2_model()
    
    # D√©marrer le chronom√®tre global
    start_time_total = time.time()
    
    try:
        # Charger le mod√®le
        print("üì• Chargement du mod√®le Florence-2-base...")
        start_load = time.time()
        
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch_dtype,
            trust_remote_code=True
        ).to(device)
        
        processor = AutoProcessor.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )
        
        model.eval()
        load_time = time.time() - start_load
        print(f"‚úÖ Mod√®le charg√© en {load_time:.2f}s\n")
        
        # Charger l'image
        print(f"üñºÔ∏è  Chargement de l'image:")
        print(f"   {IMAGE_PATH}")
        
        try:
            image = Image.open(IMAGE_PATH)
            if image.mode != 'RGB':
                image = image.convert('RGB')
            print(f"   Taille: {image.size}")
            print(f"   Mode: {image.mode}\n")
        except Exception as e:
            print(f"‚ùå Erreur chargement image: {e}")
            return
        
        # D√©marrer le chronom√®tre de traitement
        start_processing = time.time()
        
        print("=" * 80)
        print("üìã ANALYSE EN COURS...")
        print("=" * 80)
        print()
        
        # 1. Description d√©taill√©e de l'installation
        print("1Ô∏è‚É£  DESCRIPTION G√âN√âRALE DE L'INSTALLATION")
        print("-" * 80)
        task = "<MORE_DETAILED_CAPTION>"
        inputs = processor(text=task, images=image, return_tensors="pt").to(device, torch_dtype)
        
        with torch.no_grad():
            generated_ids = model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=1024,
                do_sample=False,
                num_beams=3,
            )
        
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
        parsed_answer = processor.post_process_generation(
            generated_text,
            task=task,
            image_size=(image.width, image.height)
        )
        
        description = parsed_answer.get('<MORE_DETAILED_CAPTION>', 'Non disponible')
        print(f"Description: {description}")
        print()
        
        # 2. D√©tection d'objets (pour identifier les composants)
        print("2Ô∏è‚É£  D√âTECTION DES COMPOSANTS")
        print("-" * 80)
        task = "<OD>"
        inputs = processor(text=task, images=image, return_tensors="pt").to(device, torch_dtype)
        
        with torch.no_grad():
            generated_ids = model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=1024,
                do_sample=False,
                num_beams=3,
            )
        
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
        parsed_answer = processor.post_process_generation(
            generated_text,
            task=task,
            image_size=(image.width, image.height)
        )
        
        objects = parsed_answer.get('<OD>', {})
        if isinstance(objects, dict):
            labels = objects.get('labels', [])
            print(f"Composants d√©tect√©s: {', '.join(labels) if labels else 'Aucun'}")
        print()
        
        # 3. OCR pour r√©cup√©rer le num√©ro d'√©tiquette
        print("3Ô∏è‚É£  LECTURE DU NUM√âRO D'√âTIQUETTE (OCR)")
        print("-" * 80)
        task = "<OCR_WITH_REGION>"
        inputs = processor(text=task, images=image, return_tensors="pt").to(device, torch_dtype)
        
        with torch.no_grad():
            generated_ids = model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=1024,
                do_sample=False,
                num_beams=3,
            )
        
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
        parsed_answer = processor.post_process_generation(
            generated_text,
            task=task,
            image_size=(image.width, image.height)
        )
        
        ocr_result = parsed_answer.get('<OCR_WITH_REGION>', {})
        if isinstance(ocr_result, dict):
            texts = ocr_result.get('labels', [])
            print(f"Textes d√©tect√©s: {texts}")
            
            # Chercher un num√©ro au format B.xxx.xxx.xxx.x
            import re
            etiquette_found = None
            for text in texts:
                if re.match(r'B\.\d+\.\d+\.\d+\.\d+', text):
                    etiquette_found = text
                    break
            
            if etiquette_found:
                print(f"‚úÖ Num√©ro d'√©tiquette trouv√©: {etiquette_found}")
            else:
                print(f"‚ö†Ô∏è  Num√©ro d'√©tiquette au format B.xxx.xxx.xxx.x non d√©tect√©")
        print()
        
        # 4. Analyse de densit√© de r√©gions (pour d√©tecter l'organisation)
        print("4Ô∏è‚É£  ANALYSE DES R√âGIONS D'INT√âR√äT")
        print("-" * 80)
        task = "<DENSE_REGION_CAPTION>"
        inputs = processor(text=task, images=image, return_tensors="pt").to(device, torch_dtype)
        
        with torch.no_grad():
            generated_ids = model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=1024,
                do_sample=False,
                num_beams=3,
            )
        
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
        parsed_answer = processor.post_process_generation(
            generated_text,
            task=task,
            image_size=(image.width, image.height)
        )
        
        regions = parsed_answer.get('<DENSE_REGION_CAPTION>', {})
        if isinstance(regions, dict):
            labels = regions.get('labels', [])
            print(f"R√©gions analys√©es: {len(labels)} zone(s)")
            for i, label in enumerate(labels[:5], 1):  # Afficher max 5
                print(f"   Zone {i}: {label}")
        print()
        
        # Fin du traitement
        processing_time = time.time() - start_processing
        total_time = time.time() - start_time_total
        
        # 5. √âvaluation de qualit√© (bas√©e sur l'analyse)
        print("=" * 80)
        print("‚≠ê √âVALUATION DE QUALIT√â")
        print("=" * 80)
        print()
        
        # Crit√®res d'√©valuation bas√©s sur la description
        score = 10.0
        criteres = []
        
        description_lower = description.lower()
        
        # Analyse des fibres
        if 'organized' in description_lower or 'neat' in description_lower or 'tidy' in description_lower:
            criteres.append("‚úÖ Fibres bien organis√©es (+0 point)")
        else:
            criteres.append("‚ö†Ô∏è  Organisation des fibres non confirm√©e (-1 point)")
            score -= 1
        
        # Analyse de l'alignement
        if 'straight' in description_lower or 'aligned' in description_lower:
            criteres.append("‚úÖ Installation droite (+0 point)")
        else:
            criteres.append("‚ö†Ô∏è  Alignement non confirm√© (-0.5 point)")
            score -= 0.5
        
        # V√©rification √©tiquette
        if etiquette_found:
            criteres.append(f"‚úÖ √âtiquette identifi√©e: {etiquette_found} (+0 point)")
        else:
            criteres.append("‚ö†Ô∏è  √âtiquette non d√©tect√©e (-1 point)")
            score -= 1
        
        # Analyse de la propret√©
        if 'clean' in description_lower or 'proper' in description_lower:
            criteres.append("‚úÖ Installation propre (+0 point)")
        else:
            criteres.append("‚ö†Ô∏è  Propret√© non confirm√©e (-0.5 point)")
            score -= 0.5
        
        # Goulottes
        if 'cable' in description_lower or 'conduit' in description_lower:
            criteres.append("‚úÖ Goulottes d√©tect√©es (+0 point)")
        else:
            criteres.append("‚ö†Ô∏è  Goulottes non mentionn√©es (-1 point)")
            score -= 1
        
        score = max(0, min(10, score))  # Limiter entre 0 et 10
        
        print("üìä CRIT√àRES D'√âVALUATION:")
        for critere in criteres:
            print(f"   {critere}")
        print()
        
        # Note finale
        if score >= 9:
            qualificatif = "EXCELLENT"
            emoji = "üåü"
        elif score >= 7:
            qualificatif = "TR√àS BON"
            emoji = "‚úÖ"
        elif score >= 5:
            qualificatif = "BON"
            emoji = "üëç"
        elif score >= 3:
            qualificatif = "MOYEN"
            emoji = "‚ö†Ô∏è"
        else:
            qualificatif = "INSUFFISANT"
            emoji = "‚ùå"
        
        print("=" * 80)
        print(f"{emoji}  NOTE DE QUALIT√â GLOBALE: {score:.1f}/10 - {qualificatif}")
        print("=" * 80)
        print()
        
        # Temps de traitement
        print("‚è±Ô∏è  TEMPS DE TRAITEMENT:")
        print(f"   ‚Ä¢ Chargement du mod√®le: {load_time:.2f}s")
        print(f"   ‚Ä¢ Traitement de l'image: {processing_time:.2f}s")
        print(f"   ‚Ä¢ Temps total: {total_time:.2f}s")
        print()
        
        print("=" * 80)
        print("‚úÖ ANALYSE TERMIN√âE")
        print("=" * 80)
        
    except Exception as e:
        print(f"\n‚ùå Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    analyze_ftth_installation()
