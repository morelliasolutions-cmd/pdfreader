"""
Analyse de PDF OTDR avec IA locale (LM Studio)
Extrait les donn√©es du PDF et les envoie √† LM Studio pour analyse
"""

import pdfplumber
import requests
import json
import sys
import re
from pathlib import Path
from typing import Dict, List, Optional

# Configuration LM Studio
LM_STUDIO_URL = "http://127.0.0.1:1234/v1/chat/completions"

def extract_otdr_text(pdf_path: str) -> Dict:
    """
    Extrait le texte et les tableaux d'un PDF OTDR
    
    Args:
        pdf_path: Chemin vers le fichier PDF
        
    Returns:
        Dictionnaire avec texte et tableaux extraits
    """
    result = {
        "file_name": Path(pdf_path).name,
        "text": "",
        "tables": [],
        "num_pages": 0,
        "parsed_data": {}
    }
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            result["num_pages"] = len(pdf.pages)
            
            for page_num, page in enumerate(pdf.pages, start=1):
                # Extraire le texte
                text = page.extract_text()
                if text:
                    result["text"] += f"\n=== Page {page_num} ===\n{text}\n"
                
                # Extraire les tableaux
                tables = page.extract_tables()
                if tables:
                    for table_idx, table in enumerate(tables):
                        result["tables"].append({
                            "page": page_num,
                            "table_index": table_idx,
                            "data": table
                        })
            
            # Parser les donn√©es sp√©cifiques OTDR
            result["parsed_data"] = parse_otdr_fields(result["text"])
            
    except Exception as e:
        result["error"] = str(e)
        print(f"‚ùå Erreur lors de l'extraction du PDF: {e}")
    
    return result


def parse_otdr_fields(text: str) -> Dict:
    """
    Parse les champs sp√©cifiques d'un rapport OTDR
    
    Args:
        text: Texte extrait du PDF
        
    Returns:
        Dictionnaire avec les champs pars√©s
    """
    fields = {}
    
    # Patterns de recherche
    patterns = {
        "cable_name": r"Nom C√¢ble\s*[:Ôºö]\s*([^\s]+)",
        "fiber_name": r"Nom Fibre/Num√©ro\s*[:Ôºö]\s*([^\s]+)",
        "origin": r"Origine\s*[:Ôºö]\s*([^\s]+)",
        "destination": r"Extr√©mit√©\s*[:Ôºö]\s*([^\s]+)",
        "intervention_ref": r"R√©f Intervention\s*[:Ôºö]\s*([^\s]+)",
        "operator": r"Op√©rateur\s*[:Ôºö]\s*([^\s]+)",
        "wavelength": r"(\d+)nm",
        "test_date": r"Date\s*[:Ôºö]\s*(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2})",
        "fiber_length": r"Fin de fibre\s+Km\s+.*?\s+(\d+\.\d+)",
        "total_attenuation": r"Bilan\s+dB\s+.*?\s+(\d+\.\d+)",
        "orl": r"ORL Liaison dB\s+(\d+\.\d+)",
        "avg_attenuation": r"Affai\.\s+Moy\.\s+dB/Km\s+.*?\s+(\d+\.\d+)",
        "num_events": r"Evt\s+.*?\s+(\d+)",
    }
    
    for key, pattern in patterns.items():
        match = re.search(pattern, text)
        if match:
            fields[key] = match.group(1).strip()
    
    # Extraire les √©v√©nements du tableau
    events = []
    event_pattern = r"(\d+)\s+(\d+\.\d+)\s+([-\d.]+)\s+([-\d.]+)?\s+(\d+\.\d+)\s+(\d+\.\d+)"
    for match in re.finditer(event_pattern, text):
        events.append({
            "event_num": match.group(1),
            "distance_km": match.group(2),
            "attenuation_db": match.group(3),
            "reflectance_db": match.group(4) if match.group(4) else None,
            "section_km": match.group(5),
            "cumulative_loss_db": match.group(6)
        })
    
    if events:
        fields["events"] = events
    
    return fields


def format_data_for_analysis(extracted_data: Dict) -> str:
    """
    Formate les donn√©es extraites pour l'analyse IA
    
    Args:
        extracted_data: Donn√©es extraites du PDF
        
    Returns:
        Texte format√© pour l'IA
    """
    output = f"# Rapport OTDR: {extracted_data['file_name']}\n\n"
    output += f"Nombre de pages: {extracted_data['num_pages']}\n\n"
    
    # Ajouter le texte complet
    output += "## Contenu textuel:\n"
    output += extracted_data["text"]
    
    # Ajouter les tableaux de mani√®re structur√©e
    if extracted_data["tables"]:
        output += "\n\n## Tableaux extraits:\n"
        for table_info in extracted_data["tables"]:
            output += f"\n### Tableau {table_info['table_index'] + 1} (Page {table_info['page']}):\n"
            table_data = table_info["data"]
            
            # Formater le tableau en markdown
            if table_data:
                # Header
                if table_data[0]:
                    output += "| " + " | ".join([str(cell) if cell else "" for cell in table_data[0]]) + " |\n"
                    output += "| " + " | ".join(["---"] * len(table_data[0])) + " |\n"
                
                # Rows
                for row in table_data[1:]:
                    if row:
                        output += "| " + " | ".join([str(cell) if cell else "" for cell in row]) + " |\n"
    
    return output


def analyze_with_lmstudio(data_text: str, model: str = "llama-3.2-3b-instruct") -> Dict:
    """
    Envoie les donn√©es √† LM Studio pour analyse
    
    Args:
        data_text: Donn√©es format√©es √† analyser
        model: Nom du mod√®le LM Studio
        
    Returns:
        R√©sultat de l'analyse avec score et recommandations
    """
    
    # Prompt syst√®me pour l'analyse OTDR
    system_prompt = """Tu es un expert en t√©l√©communications sp√©cialis√© dans l'analyse de mesures OTDR (Optical Time-Domain Reflectometer).

Ton r√¥le est d'analyser les rapports OTDR et de:
1. Identifier les donn√©es cl√©s (longueur de fibre, att√©nuation, longueur d'onde, √©v√©nements)
2. √âvaluer la qualit√© de la mesure
3. D√©tecter les anomalies ou probl√®mes potentiels
4. Donner un score de qualit√© sur 10
5. Fournir des recommandations

Les crit√®res d'√©valuation:
- Att√©nuation totale: < 0.5 dB/km = excellent, 0.5-1 dB/km = bon, > 1 dB/km = probl√®me
- R√©flectance aux connecteurs: < -45 dB = excellent, -45 √† -35 dB = bon, > -35 dB = probl√®me
- Pertes aux √©pissures: < 0.1 dB = excellent, 0.1-0.3 dB = acceptable, > 0.3 dB = probl√®me
- Qualit√© du trace: clean = 10, avec bruit = 6-8, tr√®s bruit√© = 0-5

R√©ponds TOUJOURS au format JSON suivant (sans markdown):
{
  "score": 8.5,
  "status": "excellent",
  "fiber_length_km": "2.45",
  "total_attenuation_db": "0.85",
  "wavelength_nm": "1550",
  "num_events": 5,
  "issues": ["Liste des probl√®mes d√©tect√©s"],
  "recommendations": ["Liste des recommandations"],
  "summary": "R√©sum√© de l'analyse en 2-3 phrases"
}"""

    user_prompt = f"""Analyse ce rapport OTDR et fournis ton √©valuation au format JSON:

{data_text}

IMPORTANT: R√©ponds UNIQUEMENT avec le JSON, sans texte avant ou apr√®s, sans balises markdown."""

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.3,  # Basse temp√©rature pour des r√©ponses plus pr√©cises
        "max_tokens": 1000
    }
    
    try:
        print("ü§ñ Envoi √† LM Studio pour analyse...")
        response = requests.post(LM_STUDIO_URL, json=payload, timeout=60)
        response.raise_for_status()
        
        result = response.json()
        
        # Extraire le contenu de la r√©ponse
        if "choices" in result and len(result["choices"]) > 0:
            content = result["choices"][0]["message"]["content"]
            
            # Nettoyer le contenu (supprimer les balises markdown si pr√©sentes)
            content = content.strip()
            if content.startswith("```json"):
                content = content[7:]
            if content.startswith("```"):
                content = content[3:]
            if content.endswith("```"):
                content = content[:-3]
            content = content.strip()
            
            # Parser le JSON
            try:
                analysis = json.loads(content)
                return analysis
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è  R√©ponse de l'IA non-JSON, tentative de parsing...")
                # Essayer d'extraire le JSON du texte
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    analysis = json.loads(json_match.group(0))
                    return analysis
                else:
                    return {
                        "score": 5.0,
                        "status": "inconnu",
                        "issues": ["Impossible de parser la r√©ponse de l'IA"],
                        "recommendations": ["V√©rifier le format de r√©ponse du mod√®le"],
                        "summary": content,
                        "raw_response": content
                    }
        else:
            return {
                "score": 0,
                "status": "erreur",
                "issues": ["Pas de r√©ponse de LM Studio"],
                "recommendations": ["V√©rifier que LM Studio est lanc√©"],
                "summary": "Erreur de communication"
            }
            
    except requests.exceptions.ConnectionError:
        return {
            "score": 0,
            "status": "erreur",
            "issues": ["Impossible de se connecter √† LM Studio"],
            "recommendations": [
                "V√©rifier que LM Studio est lanc√©",
                "V√©rifier que le serveur local est actif sur http://127.0.0.1:1234"
            ],
            "summary": "LM Studio n'est pas accessible"
        }
    except requests.exceptions.Timeout:
        return {
            "score": 0,
            "status": "erreur",
            "issues": ["Timeout de la requ√™te"],
            "recommendations": ["Le mod√®le est peut-√™tre trop lent, essayer un mod√®le plus petit"],
            "summary": "Timeout lors de l'analyse"
        }
    except Exception as e:
        return {
            "score": 0,
            "status": "erreur",
            "issues": [f"Erreur: {str(e)}"],
            "recommendations": ["V√©rifier les logs de LM Studio"],
            "summary": f"Erreur lors de l'analyse: {str(e)}"
        }


def save_results(pdf_path: str, analysis: Dict):
    """Sauvegarde les r√©sultats au format JSON"""
    output_path = Path(pdf_path).with_suffix('.analysis.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)
    print(f"üíæ R√©sultats sauvegard√©s: {output_path}")


def main():
    """Fonction principale"""
    if len(sys.argv) < 2:
        print("Usage: python analyze_otdr_with_lmstudio.py <chemin_pdf> [model]")
        print("Exemple: python analyze_otdr_with_lmstudio.py rapport_otdr.pdf")
        print("         python analyze_otdr_with_lmstudio.py rapport_otdr.pdf llama-3.2-3b-instruct")
        sys.exit(1)
    
    pdf_path = sys.argv[1]
    model = sys.argv[2] if len(sys.argv) > 2 else "llama-3.2-3b-instruct"
    
    if not Path(pdf_path).exists():
        print(f"‚ùå Erreur: Le fichier {pdf_path} n'existe pas")
        sys.exit(1)
    
    print("="*70)
    print(f"üìÑ Analyse OTDR avec IA locale")
    print("="*70)
    print(f"Fichier: {pdf_path}")
    print(f"Mod√®le IA: {model}")
    print(f"LM Studio: {LM_STUDIO_URL}")
    print("="*70)
    
    # √âtape 1: Extraction du PDF
    print("\nüìä √âTAPE 1: Extraction des donn√©es du PDF...")
    extracted_data = extract_otdr_text(pdf_path)
    
    if "error" in extracted_data:
        print(f"‚ùå √âchec de l'extraction: {extracted_data['error']}")
        sys.exit(1)
    
    print(f"   ‚úÖ {extracted_data['num_pages']} page(s) extraite(s)")
    print(f"   ‚úÖ {len(extracted_data['tables'])} tableau(x) d√©tect√©(s)")
    print(f"   ‚úÖ {len(extracted_data['text'])} caract√®res de texte")    
    # Afficher les donn√©es pars√©es
    if extracted_data.get("parsed_data"):
        print(f"\nüìã DONN√âES PARS√âES:")
        parsed = extracted_data["parsed_data"]
        if parsed.get("cable_name"):
            print(f"   - C√¢ble: {parsed['cable_name']}")
        if parsed.get("fiber_name"):
            print(f"   - Fibre: {parsed['fiber_name']}")
        if parsed.get("operator"):
            print(f"   - Op√©rateur: {parsed['operator']}")
        if parsed.get("wavelength"):
            print(f"   - Longueur d'onde: {parsed['wavelength']} nm")
        if parsed.get("fiber_length"):
            print(f"   - Longueur fibre: {parsed['fiber_length']} km")
        if parsed.get("total_attenuation"):
            print(f"   - Bilan: {parsed['total_attenuation']} dB")
        if parsed.get("events"):
            print(f"   - √âv√©nements: {len(parsed['events'])}")    
    # √âtape 2: Formatage des donn√©es
    print("\nüìù √âTAPE 2: Formatage des donn√©es pour l'IA...")
    formatted_data = format_data_for_analysis(extracted_data)
    print(f"   ‚úÖ Donn√©es format√©es ({len(formatted_data)} caract√®res)")
    
    # √âtape 3: Analyse avec LM Studio
    print("\nü§ñ √âTAPE 3: Analyse avec LM Studio...")
    analysis = analyze_with_lmstudio(formatted_data, model)
    
    # Affichage des r√©sultats
    print("\n" + "="*70)
    print("‚≠ê R√âSULTATS DE L'ANALYSE")
    print("="*70)
    
    score = analysis.get("score", 0)
    status = analysis.get("status", "inconnu")
    
    # Emoji bas√© sur le score
    if score >= 8:
        emoji = "‚úÖ"
    elif score >= 6:
        emoji = "‚úîÔ∏è"
    elif score >= 4:
        emoji = "‚ö†Ô∏è"
    else:
        emoji = "‚ùå"
    
    print(f"\n{emoji} Score: {score}/10")
    print(f"üìä Statut: {status.upper()}")
    
    # Donn√©es techniques
    if analysis.get("fiber_length_km"):
        print(f"\nüîß DONN√âES TECHNIQUES:")
        print(f"   - Longueur fibre: {analysis.get('fiber_length_km')} km")
        if analysis.get("total_attenuation_db"):
            print(f"   - Att√©nuation totale: {analysis.get('total_attenuation_db')} dB")
        if analysis.get("wavelength_nm"):
            print(f"   - Longueur d'onde: {analysis.get('wavelength_nm')} nm")
        if analysis.get("num_events"):
            print(f"   - √âv√©nements d√©tect√©s: {analysis.get('num_events')}")
    
    # R√©sum√©
    if analysis.get("summary"):
        print(f"\nüìã R√âSUM√â:")
        print(f"   {analysis['summary']}")
    
    # Probl√®mes d√©tect√©s
    if analysis.get("issues") and len(analysis["issues"]) > 0:
        print(f"\n‚ö†Ô∏è  PROBL√àMES D√âTECT√âS:")
        for issue in analysis["issues"]:
            print(f"   - {issue}")
    
    # Recommandations
    if analysis.get("recommendations") and len(analysis["recommendations"]) > 0:
        print(f"\nüí° RECOMMANDATIONS:")
        for rec in analysis["recommendations"]:
            print(f"   - {rec}")
    
    # Sauvegarde
    save_results(pdf_path, {
        "file": pdf_path,
        "model": model,
        "analysis": analysis,
        "extracted_data": {
            "num_pages": extracted_data["num_pages"],
            "num_tables": len(extracted_data["tables"]),
            "text_length": len(extracted_data["text"])
        }
    })
    
    print("\n" + "="*70)
    print("‚úÖ Analyse termin√©e!")
    print("="*70)


if __name__ == "__main__":
    main()
