"""
Script de test simple pour Microsoft Florence-2-base
Charge le mod√®le, traite une image de test et affiche le r√©sultat
"""

import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForCausalLM
import requests
import io
import os
import sys

# Configuration
MODEL_NAME = "microsoft/Florence-2-base"
device = "cpu"  # CPU par d√©faut (peut √™tre chang√© en "cuda:0" si GPU disponible)
torch_dtype = torch.float32  # float32 pour CPU


def patch_florence2_model():
    """Patch pour corriger le probl√®me _supports_sdpa avec Florence-2"""
    try:
        import transformers
        if hasattr(transformers, 'models') and hasattr(transformers.models, 'florence2'):
            try:
                from transformers.models.florence2 import modeling_florence2
                if hasattr(modeling_florence2, 'Florence2ForConditionalGeneration'):
                    if not hasattr(modeling_florence2.Florence2ForConditionalGeneration, '_supports_sdpa'):
                        setattr(modeling_florence2.Florence2ForConditionalGeneration, '_supports_sdpa', False)
                        print("‚úÖ Patch SDPA appliqu√© avec succ√®s")
            except Exception as e:
                print(f"‚ö†Ô∏è  Avertissement patch: {e}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Avertissement patch global: {e}")


def download_test_image(url, output_path="test.jpg"):
    """T√©l√©charge une image de test depuis une URL"""
    try:
        print(f"üì• T√©l√©chargement de l'image de test...")
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        image = Image.open(io.BytesIO(response.content))
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        image.save(output_path)
        print(f"‚úÖ Image sauvegard√©e: {output_path}")
        return output_path
    except Exception as e:
        print(f"‚ùå Erreur t√©l√©chargement: {e}")
        return None


def test_florence2():
    """Fonction principale de test"""
    print("=" * 60)
    print("üöÄ Test de Microsoft Florence-2-base")
    print("=" * 60)
    
    # Appliquer le patch
    patch_florence2_model()
    
    print(f"\nüìä Configuration:")
    print(f"   Device: {device}")
    print(f"   Dtype: {torch_dtype}")
    print(f"   Model: {MODEL_NAME}\n")
    
    try:
        # Charger le mod√®le
        print("üì• Chargement du mod√®le et du processeur...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch_dtype,
            trust_remote_code=True
        ).to(device)
        
        processor = AutoProcessor.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )
        
        model.eval()
        print("‚úÖ Mod√®le charg√© avec succ√®s\n")
        
        # Pr√©parer l'image de test
        test_image_path = "test.jpg"
        
        if not os.path.exists(test_image_path):
            # T√©l√©charger une image de test
            test_url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg"
            test_image_path = download_test_image(test_url, test_image_path)
            
            if not test_image_path:
                print("‚ùå Impossible de charger l'image de test")
                return
        
        print(f"üñºÔ∏è  Chargement de l'image: {test_image_path}")
        image = Image.open(test_image_path)
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        print(f"   Taille: {image.size}\n")
        
        # Tester diff√©rentes t√¢ches
        tasks = [
            ("<CAPTION>", "Description courte"),
            ("<DETAILED_CAPTION>", "Description d√©taill√©e"),
            ("<MORE_DETAILED_CAPTION>", "Description tr√®s d√©taill√©e"),
        ]
        
        for task_prompt, task_name in tasks:
            print(f"üìã {task_name} ({task_prompt})")
            print("-" * 60)
            
            try:
                # Pr√©parer les inputs
                inputs = processor(
                    text=task_prompt,
                    images=image,
                    return_tensors="pt"
                ).to(device, torch_dtype)
                
                # G√©n√©rer
                with torch.no_grad():
                    generated_ids = model.generate(
                        input_ids=inputs["input_ids"],
                        pixel_values=inputs["pixel_values"],
                        max_new_tokens=1024,
                        do_sample=False,
                        num_beams=3,
                    )
                
                # D√©coder
                generated_text = processor.batch_decode(
                    generated_ids,
                    skip_special_tokens=False
                )[0]
                
                # Parser
                parsed_answer = processor.post_process_generation(
                    generated_text,
                    task=task_prompt,
                    image_size=(image.width, image.height)
                )
                
                print(f"‚úÖ R√©sultat: {parsed_answer}")
                print()
                
            except Exception as e:
                print(f"‚ùå Erreur: {e}\n")
        
        print("=" * 60)
        print("‚úÖ Test termin√© avec succ√®s!")
        print("=" * 60)
        
    except Exception as e:
        print(f"\n‚ùå Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    test_florence2()
